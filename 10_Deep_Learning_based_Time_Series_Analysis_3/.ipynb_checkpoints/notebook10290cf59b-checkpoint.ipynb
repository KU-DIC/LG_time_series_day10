{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ LG전자_DX_Intensive_Course  ]<br><br> 딥러닝 기반 시계열 분석 3: CNN 주요 모델 2 & CAM/Grad-CAM <br>\n",
    "### ** Key Points of Using Pretrained CNN Models\n",
    "- 1. 데이터 구축<br>\n",
    "    - Pretrained model은 사전에 설정한 input size에 적합한 구조를 가지고 있으므로 새로운 데이터를 pretrained model의 input size로 변환해야 함 <br>\n",
    "- 2. 모델 구축<br>\n",
    "    - Pretrained model은 ImageNet에 학습되어 1000개의 output node를 가지고 있으므로 해당 모델을 새로운 데이터에 사용하기 위해 pretrained model의 마지막 layer를 새로운 데이터의 class 개수에 맞는 layer로 대체해야 함 <br>\n",
    "- 3. 모델 학습<br>\n",
    "    - Pretrained model의 weight를 그대로 사용할 layer와 새로운 데이터에 맞게 업데이트할 layer를 구분하여 optimizer가 업데이트할 weight를 설정해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br>0. Hyperparameter Setting\n",
    "- data_dir: Top level data directory. Here we assume the format of the directory conforms to the ImageFolder structure\n",
    "- model_name: Models to choose from [inception, resnet, densenet]\n",
    "- batch_size: Batch size for training (change depending on how much memory you have)\n",
    "- num_classes: Number of classes in the dataset\n",
    "- num_epochs: Number of epochs to train for\n",
    "- feature_extract: Flag for feature extracting. When False, we finetune the whole model, when True we only update the reshaped layer params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T09:59:33.866348Z",
     "iopub.status.busy": "2022-01-04T09:59:33.866008Z",
     "iopub.status.idle": "2022-01-04T09:59:34.424735Z",
     "shell.execute_reply": "2022-01-04T09:59:34.423940Z",
     "shell.execute_reply.started": "2022-01-04T09:59:33.866259Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모듈 불러오기\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T09:59:34.427005Z",
     "iopub.status.busy": "2022-01-04T09:59:34.426704Z",
     "iopub.status.idle": "2022-01-04T09:59:34.455084Z",
     "shell.execute_reply": "2022-01-04T09:59:34.454268Z",
     "shell.execute_reply.started": "2022-01-04T09:59:34.426967Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter setting\n",
    "data_dir = \"../input/hymenoptera-data/hymenoptera_data\"\n",
    "model_name = \"resnet\"\n",
    "batch_size = 32\n",
    "num_classes = 2\n",
    "num_epochs = 15\n",
    "feature_extract = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Detect if we have a GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T09:59:34.457082Z",
     "iopub.status.busy": "2022-01-04T09:59:34.456566Z",
     "iopub.status.idle": "2022-01-04T09:59:34.465557Z",
     "shell.execute_reply": "2022-01-04T09:59:34.464763Z",
     "shell.execute_reply.started": "2022-01-04T09:59:34.457042Z"
    }
   },
   "outputs": [],
   "source": [
    "input_size_dict = {\"inception\": 299, \"resnet\": 224, \"densenet\": 224}\n",
    "input_size = input_size_dict[\"resnet\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br><br>__1. Data: Hymenoptera Data__\n",
    "- 데이터 description <br>\n",
    "    - Hymenoptera Data는 개미와 벌의 이미지로 구성된 데이터셋이다. 해당 데이터셋은 총 398개의 이미지와 해당 이미지에 매칭되는 개미/벌의 label로 구성되어 있으며, 이 중 245개는 train 데이터이고 153개는 test 데이터이다. Hymenoptera Data를 활용한 이미지 분류 task는 이미지를 input으로 받아 이를 개미/벌 중 하나의 class로 분류하는 것을 목표로 한다. (출처: https://www.kaggle.com/ajayrana/hymenoptera-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1-1. Data Transform 설정하기\n",
    "주의) train 데이터에는 data augmentation을 적용하여 한 이미지로부터 다양한 이미지를 추출하여 학습함으로써 학습의 효과를 높이지만, validation 데이터에는 input size를 맞추기 위한 변환과 normalization만 적용함\n",
    "- transforms.RandomResizedCrop(input_size): 이미지의 일부 영역을 랜덤으로 crop한 후, 이를 input_size의 크기로 resize함\n",
    "- transforms.RandomHorizontalFlip(): 50%의 확률로 이미지를 좌우 반전함\n",
    "- transforms.Resize(input_size): 이미지를 input_size의 크기로 resize함\n",
    "- transforms.CenterCrop(input_size): 이미지의 중심부에서 input_size의 크기의 영역을 crop함\n",
    "- transforms.ToTensor(): 0-1 사이의 값을 가진 Tensor 형태로 변환함\n",
    "- transforms.Normalize(mean, std): mean을 빼고 std로 나누는 정규화를 적용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T09:59:34.467022Z",
     "iopub.status.busy": "2022-01-04T09:59:34.466687Z",
     "iopub.status.idle": "2022-01-04T09:59:34.475240Z",
     "shell.execute_reply": "2022-01-04T09:59:34.474380Z",
     "shell.execute_reply.started": "2022-01-04T09:59:34.466987Z"
    }
   },
   "outputs": [],
   "source": [
    "# input 데이터에 적용할 transform 설정하기\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1-2. Dataset &  DataLoader 구축하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T09:59:34.479081Z",
     "iopub.status.busy": "2022-01-04T09:59:34.478887Z",
     "iopub.status.idle": "2022-01-04T09:59:34.495412Z",
     "shell.execute_reply": "2022-01-04T09:59:34.494795Z",
     "shell.execute_reply.started": "2022-01-04T09:59:34.479055Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset 구축하기\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T09:59:34.498139Z",
     "iopub.status.busy": "2022-01-04T09:59:34.497676Z",
     "iopub.status.idle": "2022-01-04T09:59:34.504116Z",
     "shell.execute_reply": "2022-01-04T09:59:34.503156Z",
     "shell.execute_reply.started": "2022-01-04T09:59:34.498103Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataloader 구축하기\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br><br>__2. Model: GoogLeNet, ResNet, DenseNet__\n",
    "## 2-1. GoogLeNet (Inception V3)\n",
    "- Fine-tuning을 위해 auxiliary output을 도출하는 **(AuxLogits.fc)** layer와 primary output을 도출하는 **(fc)** layer를 변경해야 함<br>\n",
    "- Inveption V3는 다른 pretrained CNN models와 다르게 auxiliary task를 수행하는 layer를 가지므로 해당 layer도 변경해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T09:59:34.506108Z",
     "iopub.status.busy": "2022-01-04T09:59:34.505598Z",
     "iopub.status.idle": "2022-01-04T09:59:34.909125Z",
     "shell.execute_reply": "2022-01-04T09:59:34.908388Z",
     "shell.execute_reply.started": "2022-01-04T09:59:34.506071Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pretrained Inveption V3 불러오기\n",
    "inception = torchvision.models.inception_v3(pretrained=True)\n",
    "print(inception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T09:59:34.911062Z",
     "iopub.status.busy": "2022-01-04T09:59:34.910620Z",
     "iopub.status.idle": "2022-01-04T09:59:34.921487Z",
     "shell.execute_reply": "2022-01-04T09:59:34.920477Z",
     "shell.execute_reply.started": "2022-01-04T09:59:34.911024Z"
    }
   },
   "outputs": [],
   "source": [
    "# (AuxLogits.fc) layer와 (fc) layer 변경하기\n",
    "inception.AuxLogits.fc = nn.Linear(768, num_classes)\n",
    "inception.fc = nn.Linear(2048, num_classes)\n",
    "print(inception)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <br>2-2. ResNet\n",
    "- Fine-tuning을 위해 output을 도출하는 **(fc)** layer를 변경해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T09:59:34.923336Z",
     "iopub.status.busy": "2022-01-04T09:59:34.923023Z",
     "iopub.status.idle": "2022-01-04T09:59:35.134010Z",
     "shell.execute_reply": "2022-01-04T09:59:35.133270Z",
     "shell.execute_reply.started": "2022-01-04T09:59:34.923296Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pretrained Resnet18 불러오기\n",
    "resnet = torchvision.models.resnet18(pretrained=True)\n",
    "print(resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T09:59:35.135939Z",
     "iopub.status.busy": "2022-01-04T09:59:35.135120Z",
     "iopub.status.idle": "2022-01-04T09:59:35.141786Z",
     "shell.execute_reply": "2022-01-04T09:59:35.141106Z",
     "shell.execute_reply.started": "2022-01-04T09:59:35.135902Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (fc) layer 변경하기\n",
    "resnet.fc = nn.Linear(512, num_classes)\n",
    "print(resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <br>2-3. DenseNet\n",
    "- Fine-tuning을 위해 output을 도출하는 **(classifier)** layer를 변경해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T09:59:35.143613Z",
     "iopub.status.busy": "2022-01-04T09:59:35.143140Z",
     "iopub.status.idle": "2022-01-04T09:59:35.392683Z",
     "shell.execute_reply": "2022-01-04T09:59:35.391973Z",
     "shell.execute_reply.started": "2022-01-04T09:59:35.143576Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pretrained Densenet121 불러오기\n",
    "densenet = torchvision.models.densenet121(pretrained=True)\n",
    "print(densenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T09:59:35.394340Z",
     "iopub.status.busy": "2022-01-04T09:59:35.393967Z",
     "iopub.status.idle": "2022-01-04T09:59:35.404599Z",
     "shell.execute_reply": "2022-01-04T09:59:35.403826Z",
     "shell.execute_reply.started": "2022-01-04T09:59:35.394306Z"
    }
   },
   "outputs": [],
   "source": [
    "# (classifier) layer 변경하기\n",
    "densenet.classifier = nn.Linear(1024, num_classes)\n",
    "print(densenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <br>2-4. Fine-tuning을 위한 모델 구축 및 학습 parameter 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T09:59:35.406167Z",
     "iopub.status.busy": "2022-01-04T09:59:35.405694Z",
     "iopub.status.idle": "2022-01-04T09:59:35.410688Z",
     "shell.execute_reply": "2022-01-04T09:59:35.409908Z",
     "shell.execute_reply.started": "2022-01-04T09:59:35.406130Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T09:59:35.414671Z",
     "iopub.status.busy": "2022-01-04T09:59:35.414057Z",
     "iopub.status.idle": "2022-01-04T09:59:35.423514Z",
     "shell.execute_reply": "2022-01-04T09:59:35.422700Z",
     "shell.execute_reply.started": "2022-01-04T09:59:35.414634Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    model_ft = None\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet121\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "\n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T09:59:35.424948Z",
     "iopub.status.busy": "2022-01-04T09:59:35.424625Z",
     "iopub.status.idle": "2022-01-04T09:59:37.445834Z",
     "shell.execute_reply": "2022-01-04T09:59:37.445058Z",
     "shell.execute_reply.started": "2022-01-04T09:59:35.424910Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the model for model_name\n",
    "model_ft = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "model_ft = model_ft.to(device)\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br><br>__3. Training the Pretrained CNN Model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3-1. Optimizer 구축하기\n",
    "- Finetuning: 모델의 모든 parameter를 업데이트함\n",
    "- Feature extraction: 초기화한 layer의 parameter만 업데이트하고 나머지는 고정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T09:59:37.447540Z",
     "iopub.status.busy": "2022-01-04T09:59:37.447128Z",
     "iopub.status.idle": "2022-01-04T09:59:37.455817Z",
     "shell.execute_reply": "2022-01-04T09:59:37.454872Z",
     "shell.execute_reply.started": "2022-01-04T09:59:37.447503Z"
    }
   },
   "outputs": [],
   "source": [
    "# 업데이트 할 parameter 설정하기\n",
    "params_to_update = model_ft.parameters()\n",
    "\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name, param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"-\", name)\n",
    "else:\n",
    "    for name, param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"-\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T09:59:37.457616Z",
     "iopub.status.busy": "2022-01-04T09:59:37.457148Z",
     "iopub.status.idle": "2022-01-04T09:59:37.465852Z",
     "shell.execute_reply": "2022-01-04T09:59:37.465167Z",
     "shell.execute_reply.started": "2022-01-04T09:59:37.457579Z"
    }
   },
   "outputs": [],
   "source": [
    "# params_to_update의 parameter만 업데이트 할 SGD optimizer 구축하기\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3-2. 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T09:59:37.467627Z",
     "iopub.status.busy": "2022-01-04T09:59:37.467145Z",
     "iopub.status.idle": "2022-01-04T09:59:37.483175Z",
     "shell.execute_reply": "2022-01-04T09:59:37.482467Z",
     "shell.execute_reply.started": "2022-01-04T09:59:37.467590Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T09:59:37.486492Z",
     "iopub.status.busy": "2022-01-04T09:59:37.485727Z",
     "iopub.status.idle": "2022-01-04T09:59:37.494099Z",
     "shell.execute_reply": "2022-01-04T09:59:37.493365Z",
     "shell.execute_reply.started": "2022-01-04T09:59:37.486451Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss function 설정하기\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 모델 학습하기\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br><br>__4. GradCAM__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T10:04:45.986561Z",
     "iopub.status.busy": "2022-01-04T10:04:45.985392Z",
     "iopub.status.idle": "2022-01-04T10:04:45.993264Z",
     "shell.execute_reply": "2022-01-04T10:04:45.992112Z",
     "shell.execute_reply.started": "2022-01-04T10:04:45.986512Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모듈 불러오기\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from gradcam.utils import visualize_cam\n",
    "from gradcam import GradCAM, GradCAMpp\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T09:59:45.136609Z",
     "iopub.status.busy": "2022-01-04T09:59:45.136032Z",
     "iopub.status.idle": "2022-01-04T09:59:45.401027Z",
     "shell.execute_reply": "2022-01-04T09:59:45.400310Z",
     "shell.execute_reply.started": "2022-01-04T09:59:45.136569Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test 이미지 불러오기\n",
    "img_path = \"../input/hymenoptera-data/hymenoptera_data/val/bees/6a00d8341c630a53ef00e553d0beb18834-800wi.jpg\"\n",
    "img = Image.open(img_path)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T09:59:46.919867Z",
     "iopub.status.busy": "2022-01-04T09:59:46.919110Z",
     "iopub.status.idle": "2022-01-04T09:59:46.932571Z",
     "shell.execute_reply": "2022-01-04T09:59:46.931820Z",
     "shell.execute_reply.started": "2022-01-04T09:59:46.919824Z"
    }
   },
   "outputs": [],
   "source": [
    "# input에 transform 적용하기 \n",
    "normed_img = data_transforms['val'](img)[None].to(device)\n",
    "normed_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T10:06:01.418834Z",
     "iopub.status.busy": "2022-01-04T10:06:01.418059Z",
     "iopub.status.idle": "2022-01-04T10:06:01.545027Z",
     "shell.execute_reply": "2022-01-04T10:06:01.544404Z",
     "shell.execute_reply.started": "2022-01-04T10:06:01.418786Z"
    }
   },
   "outputs": [],
   "source": [
    "torch_img = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])(img).to(device)\n",
    "normed_torch_img = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(torch_img)[None]\n",
    "\n",
    "configs = [\n",
    "    dict(model_type='resnet', arch=resnet, layer_name='layer4')\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    config['arch'].to(device).eval()\n",
    "\n",
    "cams = [\n",
    "    [cls.from_config(**config) for cls in (GradCAM, GradCAMpp)]\n",
    "    for config in configs\n",
    "]\n",
    "\n",
    "images = []\n",
    "for gradcam, gradcam_pp in cams:\n",
    "    mask, _ = gradcam(normed_torch_img)\n",
    "    heatmap, result = visualize_cam(mask, torch_img)\n",
    "\n",
    "    mask_pp, _ = gradcam_pp(normed_torch_img)\n",
    "    heatmap_pp, result_pp = visualize_cam(mask_pp, torch_img)\n",
    "    \n",
    "    images.extend([torch_img.cpu(), heatmap, heatmap_pp, result, result_pp])\n",
    "    \n",
    "grid_image = make_grid(images, ncol=1)\n",
    "transforms.ToPILImage()(grid_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
